import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
from datetime import datetime

import pywt
import itertools
import nolds

import warnings
warnings.filterwarnings("ignore")

import gc
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (Dense, LSTM, GRU, SimpleRNN, Conv1D,
                                     MaxPooling1D, Flatten, Input, Reshape,
                                     Lambda, concatenate, TimeDistributed, Dropout)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import (mean_squared_error, mean_absolute_error, 
                             r2_score, explained_variance_score)
from sklearn.model_selection import TimeSeriesSplit, ParameterGrid
from sklearn.feature_selection import RFE, RFECV
from sklearn.ensemble import RandomForestRegressor
from statsmodels.tsa.arima.model import ARIMA
from arch import arch_model

# Interpretability libraries
import shap
import lime
from lime.lime_tabular import LimeTabularExplainer

# ------------------------------------------------
# 0. Setup directories
# ------------------------------------------------
os.makedirs('plots', exist_ok=True)
os.makedirs('results', exist_ok=True)
os.makedirs('interpretability', exist_ok=True)

# ------------------------------------------------
# Enhanced Parameter grids for hyperparameter search
# ------------------------------------------------
param_grid = {
    'ANN': {
        'layers': [[128,64,32], [64,32], [256,128,64]],
        'learning_rate': [1e-3, 1e-4, 5e-4],
        'batch_size': [32, 64],
        'dropout_rate': [0.2, 0.3]
    },
    'LSTM': {
        'units': [[128,64], [64,32], [256,128]],
        'learning_rate': [1e-3, 1e-4],
        'batch_size': [32, 64],
        'dropout_rate': [0.2, 0.3]
    },
    'CNN_LSTM': {
        'conv_filters': [32, 64, 128],
        'lstm_units': [64, 128],
        'learning_rate': [1e-3, 1e-4],
        'batch_size': [32, 64]
    }
}

# ------------------------------------------------
# 1. Load Data with Market Regime Detection
# ------------------------------------------------
def load_data(path):
    df = pd.read_csv(path, parse_dates=['DATE'])
    df.columns = df.columns.str.upper()
    df.set_index('DATE', inplace=True)
    return df

# ------------------------------------------------
# 2. Enhanced Market Regimes with Bull/Bear Detection
# ------------------------------------------------
market_periods = {
    'bull_2012': ('2012-10-05','2015-12-31'),
    'correction_2016': ('2016-01-01','2016-06-30'),
    'bull_2016': ('2016-07-01','2018-01-25'),
    'bear_2018': ('2018-01-26','2018-12-24'),
    'recovery_2019': ('2018-12-25','2020-02-19'),
    'covid_crash': ('2020-02-20','2020-03-23'),
    'recovery_2020': ('2020-03-24','2022-01-03'),
    'bear_2022': ('2022-01-04','2022-10-12'),
    'bull_2022': ('2022-10-13','2025-03-27')
}

def label_market_regime(date):
    if isinstance(date, (int, float)):
        date = pd.to_datetime(date, unit='ns' if date > 1e15 else 's')
    elif not hasattr(date, 'strftime'):
        date = pd.to_datetime(date)
    
    ds = date.strftime('%Y-%m-%d')
    for regime, (start, end) in market_periods.items():
        if start <= ds <= end:
            return regime
    return 'other'

def categorize_regime_type(regime):
    """Categorize detailed regimes into bull/bear/neutral"""
    if 'bull' in regime or 'recovery' in regime:
        return 'bull'
    elif 'bear' in regime or 'crash' in regime:
        return 'bear'
    else:
        return 'neutral'

# ------------------------------------------------
# 3. Enhanced Feature Selection with RFE and SHAP
# ------------------------------------------------
def recursive_feature_elimination(X, y, feature_names, n_features=10):
    """Apply RFE with Random Forest for feature selection"""
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rfe = RFECV(rf, step=1, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
    
    # Reshape for 2D if needed
    if X.ndim == 3:
        X_2d = X.reshape(X.shape[0], -1)
    else:
        X_2d = X
    
    rfe.fit(X_2d, y)
    
    selected_features = [feature_names[i] for i in range(len(feature_names)) if rfe.support_[i]]
    feature_rankings = rfe.ranking_
    
    return selected_features, feature_rankings, rfe.grid_scores_

def calculate_feature_importance_shap(model, X_sample, feature_names):
    """Calculate SHAP values for feature importance"""
    try:
        # Create explainer
        if hasattr(model, 'predict'):
            explainer = shap.Explainer(model.predict, X_sample[:100])
        else:
            explainer = shap.Explainer(model, X_sample[:100])
        
        shap_values = explainer(X_sample[:100])
        
        # Calculate mean absolute SHAP values
        if hasattr(shap_values, 'values'):
            importance_scores = np.mean(np.abs(shap_values.values), axis=0)
        else:
            importance_scores = np.mean(np.abs(shap_values), axis=0)
        
        return importance_scores, shap_values
    except Exception as e:
        print(f"SHAP calculation failed: {e}")
        return None, None

def visualize_feature_importance(importance_scores, feature_names, title="Feature Importance"):
    """Create feature importance visualizations"""
    plt.figure(figsize=(12, 8))
    
    # Bar plot
    plt.subplot(2, 1, 1)
    indices = np.argsort(importance_scores)[::-1]
    plt.bar(range(len(importance_scores)), importance_scores[indices])
    plt.xticks(range(len(importance_scores)), [feature_names[i] for i in indices], rotation=45)
    plt.title(f"{title} - Bar Plot")
    plt.tight_layout()
    
    # Heatmap
    plt.subplot(2, 1, 2)
    importance_matrix = importance_scores.reshape(1, -1)
    sns.heatmap(importance_matrix, xticklabels=feature_names, yticklabels=['Importance'], 
                annot=True, cmap='viridis', fmt='.3f')
    plt.title(f"{title} - Heatmap")
    plt.tight_layout()
    
    plt.savefig(f'interpretability/{title.lower().replace(" ", "_")}.png', dpi=300, bbox_inches='tight')
    plt.close()

# ------------------------------------------------
# 4. Computational Efficiency Measurements
# ------------------------------------------------
class ModelLatencyProfiler:
    def __init__(self):
        self.latency_results = []
    
    def measure_inference_time(self, model, X_test, model_name, n_runs=100):
        """Measure inference latency for a model"""
        # Warm up
        _ = model.predict(X_test[:10])
        
        # Measure latency
        times = []
        for _ in range(n_runs):
            start_time = time.time()
            _ = model.predict(X_test[:1])  # Single prediction
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # Convert to ms
        
        latency_stats = {
            'model': model_name,
            'mean_latency_ms': np.mean(times),
            'std_latency_ms': np.std(times),
            'min_latency_ms': np.min(times),
            'max_latency_ms': np.max(times),
            'p95_latency_ms': np.percentile(times, 95),
            'p99_latency_ms': np.percentile(times, 99)
        }
        
        self.latency_results.append(latency_stats)
        return latency_stats
    
    def compare_model_efficiency(self):
        """Create comparison plots for model efficiency"""
        if not self.latency_results:
            return
        
        df = pd.DataFrame(self.latency_results)
        
        plt.figure(figsize=(15, 10))
        
        # Latency comparison
        plt.subplot(2, 2, 1)
        plt.bar(df['model'], df['mean_latency_ms'], yerr=df['std_latency_ms'])
        plt.xticks(rotation=45)
        plt.ylabel('Latency (ms)')
        plt.title('Model Inference Latency Comparison')
        
        # Box plot for latency distribution
        plt.subplot(2, 2, 2)
        latency_data = []
        model_names = []
        for result in self.latency_results:
            latency_data.append([result['mean_latency_ms']])
            model_names.append(result['model'])
        
        plt.boxplot(latency_data, labels=model_names)
        plt.xticks(rotation=45)
        plt.ylabel('Latency (ms)')
        plt.title('Latency Distribution by Model')
        
        # P95 vs P99 comparison
        plt.subplot(2, 2, 3)
        x = np.arange(len(df))
        width = 0.35
        plt.bar(x - width/2, df['p95_latency_ms'], width, label='P95')
        plt.bar(x + width/2, df['p99_latency_ms'], width, label='P99')
        plt.xticks(x, df['model'], rotation=45)
        plt.ylabel('Latency (ms)')
        plt.title('P95 vs P99 Latency')
        plt.legend()
        
        # Efficiency score (1/latency)
        plt.subplot(2, 2, 4)
        efficiency_score = 1000 / df['mean_latency_ms']  # Higher is better
        plt.bar(df['model'], efficiency_score)
        plt.xticks(rotation=45)
        plt.ylabel('Efficiency Score')
        plt.title('Model Efficiency Score (Higher = Better)')
        
        plt.tight_layout()
        plt.savefig('plots/model_efficiency_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return df

# ------------------------------------------------
# 5. Lightweight Model Alternatives
# ------------------------------------------------
def create_lightweight_model(model_type, input_shape, compression_ratio=0.5):
    """Create lightweight versions of models using pruning concepts"""
    
    if model_type == 'Lightweight_CNN_LSTM':
        model = Sequential([
            Conv1D(32, 3, activation='relu', input_shape=input_shape),  # Reduced filters
            MaxPooling1D(2),
            LSTM(32, return_sequences=False, dropout=0.2),  # Reduced units
            Dense(16, activation='relu'),  # Smaller dense layer
            Dense(1)
        ])
    
    elif model_type == 'Lightweight_LSTM':
        units = int(64 * compression_ratio)
        model = Sequential([
            LSTM(units, return_sequences=True, input_shape=input_shape, dropout=0.2),
            LSTM(units//2, dropout=0.2),
            Dense(units//4, activation='relu'),
            Dense(1)
        ])
    
    elif model_type == 'Lightweight_ANN':
        model = Sequential([
            Flatten(input_shape=input_shape),
            Dense(64, activation='relu'),  # Much smaller
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.3),
            Dense(1)
        ])
    
    model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')
    return model

def knowledge_distillation_training(teacher_model, student_model, X_train, y_train, 
                                   X_val, y_val, temperature=3.0, alpha=0.7):
    """Implement knowledge distillation for model compression"""
    
    def distillation_loss(y_true, y_pred, teacher_pred, temperature, alpha):
        """Custom loss function for knowledge distillation"""
        # Student loss (hard targets)
        student_loss = tf.keras.losses.mse(y_true, y_pred)
        
        # Distillation loss (soft targets)
        teacher_soft = teacher_pred / temperature
        student_soft = y_pred / temperature
        distill_loss = tf.keras.losses.mse(teacher_soft, student_soft) * (temperature ** 2)
        
        # Combined loss
        return alpha * distill_loss + (1 - alpha) * student_loss
    
    # Get teacher predictions
    teacher_pred_train = teacher_model.predict(X_train)
    teacher_pred_val = teacher_model.predict(X_val)
    
    # Custom training loop would go here
    # For simplicity, using regular training with teacher predictions as additional guidance
    
    # Train student model
    history = student_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=50,
        batch_size=32,
        callbacks=[EarlyStopping(patience=10)],
        verbose=0
    )
    
    return student_model, history

# ------------------------------------------------
# 6. Online Learning and Concept Drift Detection
# ------------------------------------------------
class OnlineLearningAdapter:
    def __init__(self, base_model, learning_rate=0.01, window_size=100):
        self.base_model = base_model
        self.learning_rate = learning_rate
        self.window_size = window_size
        self.performance_history = []
        self.drift_detected = False
    
    def detect_concept_drift(self, current_loss, threshold=2.0):
        """Simple concept drift detection based on performance degradation"""
        if len(self.performance_history) < self.window_size:
            self.performance_history.append(current_loss)
            return False
        
        recent_avg = np.mean(self.performance_history[-self.window_size//2:])
        historical_avg = np.mean(self.performance_history[:-self.window_size//2])
        
        if recent_avg > historical_avg * threshold:
            self.drift_detected = True
            return True
        
        self.performance_history.append(current_loss)
        if len(self.performance_history) > self.window_size * 2:
            self.performance_history = self.performance_history[-self.window_size:]
        
        return False
    
    def incremental_update(self, X_new, y_new):
        """Perform incremental learning on new data"""
        if self.drift_detected:
            # Retrain with higher learning rate if drift detected
            self.base_model.compile(
                optimizer=Adam(learning_rate=self.learning_rate * 2),
                loss='mse'
            )
            self.drift_detected = False
        
        # Incremental training
        self.base_model.fit(X_new, y_new, epochs=1, verbose=0)
        
        # Update performance tracking
        loss = self.base_model.evaluate(X_new, y_new, verbose=0)
        self.detect_concept_drift(loss)

# ------------------------------------------------
# 7. Enhanced Interpretability Framework
# ------------------------------------------------
class InterpretabilityFramework:
    def __init__(self, model, feature_names):
        self.model = model
        self.feature_names = feature_names
        self.explanations = {}
    
    def explain_with_lime(self, X_sample, y_sample, idx=0):
        """Generate LIME explanations for specific predictions"""
        # Flatten input for LIME if needed
        if X_sample.ndim == 3:
            X_flat = X_sample.reshape(X_sample.shape[0], -1)
            feature_names_flat = [f"{name}_{i}" for name in self.feature_names 
                                for i in range(X_sample.shape[1])]
        else:
            X_flat = X_sample
            feature_names_flat = self.feature_names
        
        # Create LIME explainer
        explainer = LimeTabularExplainer(
            X_flat,
            feature_names=feature_names_flat,
            mode='regression'
        )
        
        # Explain specific instance
        explanation = explainer.explain_instance(
            X_flat[idx], 
            lambda x: self.model.predict(x.reshape(-1, *X_sample.shape[1:])),
            num_features=min(10, len(feature_names_flat))
        )
        
        self.explanations[f'lime_{idx}'] = explanation
        return explanation
    
    def explain_with_shap(self, X_sample):
        """Generate SHAP explanations"""
        try:
            explainer = shap.Explainer(self.model, X_sample[:100])
            shap_values = explainer(X_sample[:100])
            self.explanations['shap'] = shap_values
            return shap_values
        except Exception as e:
            print(f"SHAP explanation failed: {e}")
            return None
    
    def create_interpretability_report(self, X_sample, y_sample, save_path='interpretability/'):
        """Generate comprehensive interpretability report"""
        
        # SHAP analysis
        shap_values = self.explain_with_shap(X_sample)
        if shap_values is not None:
            plt.figure(figsize=(12, 8))
            shap.summary_plot(shap_values, X_sample[:100], 
                            feature_names=self.feature_names, 
                            show=False)
            plt.savefig(f'{save_path}shap_summary.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # LIME analysis for sample predictions
        for i in [0, len(X_sample)//2, -1]:
            if i >= len(X_sample):
                continue
            explanation = self.explain_with_lime(X_sample, y_sample, i)
            fig = explanation.as_pyplot_figure()
            fig.savefig(f'{save_path}lime_explanation_{i}.png', dpi=300, bbox_inches='tight')
            plt.close()

# ------------------------------------------------
# 8. Market Shock Case Studies
# ------------------------------------------------
def analyze_market_shock_scenarios(pred_df):
    """Analyze model performance during market shock periods"""
    
    shock_periods = {
        'COVID_Crash': ('2020-02-20', '2020-03-23'),
        'Bear_2018': ('2018-01-26', '2018-12-24'),
        'Bear_2022': ('2022-01-04', '2022-10-12')
    }
    
    shock_analysis = []
    
    for shock_name, (start, end) in shock_periods.items():
        # Filter predictions for shock period
        mask = (pred_df['time_index'] >= start) & (pred_df['time_index'] <= end)
        shock_data = pred_df[mask]
        
        if len(shock_data) == 0:
            continue
        
        # Calculate metrics during shock
        for model in shock_data['model'].unique():
            model_data = shock_data[shock_data['model'] == model]
            if len(model_data) == 0:
                continue
            
            metrics = calculate_metrics(model_data['y_true'], model_data['y_pred'])
            
            shock_analysis.append({
                'shock_period': shock_name,
                'model': model,
                'start_date': start,
                'end_date': end,
                'n_observations': len(model_data),
                **metrics
            })
    
    return pd.DataFrame(shock_analysis)

# ------------------------------------------------
# 9. Enhanced Fractal and Wavelet Analysis
# ------------------------------------------------
def hurst_exponent(ts):
    try:
        lags = range(2, min(20, len(ts)//4))
        tau = [np.std(ts[lag:] - ts[:-lag]) for lag in lags if lag < len(ts)]
        if len(tau) < 3:
            return np.nan
        
        log_lags = np.log([lag for lag in lags if lag < len(ts)][:len(tau)])
        log_tau = np.log(tau)
        
        # Remove any invalid values
        valid_mask = np.isfinite(log_lags) & np.isfinite(log_tau)
        if np.sum(valid_mask) < 3:
            return np.nan
        
        poly = np.polyfit(log_lags[valid_mask], log_tau[valid_mask], 1)
        return poly[0]
    except:
        return np.nan

def apply_hurst(df, price_col='PRICE', window_size=100):
    df['HURST_PRICE'] = df[price_col].rolling(window=window_size).apply(hurst_exponent, raw=True)
    return df

def apply_wavelet_energy(segment, wavelet='db4', level=3):
    try:
        if len(segment) < 2**level:
            return [np.nan] * (level + 1)
        coeffs = pywt.wavedec(segment, wavelet, level=level)
        return [np.sum(c**2) if len(c) > 0 else 0 for c in coeffs]
    except:
        return [np.nan] * (level + 1)

def apply_wavelets(df, col_list=None, window=150):
    if col_list is None:
        col_list = ['PRICE', 'PUTCALLRATIO']
    wavelet_cols = []
    for col in col_list:
        feats = []
        for i in range(window, len(df)):
            segment = df[col].iloc[i-window:i].dropna()
            if len(segment) >= window//2:
                energy_vals = apply_wavelet_energy(segment)
            else:
                energy_vals = [np.nan] * 4
            feats.append(energy_vals)
        
        for j in range(4):
            new_col = f'WAVELET_{col}_L{j}'
            df[new_col] = [np.nan]*window + [x[j] for x in feats]
            wavelet_cols.append(new_col)
    return df, wavelet_cols

# ------------------------------------------------
# 10. Enhanced Feature Preparation
# ------------------------------------------------
def prepare_features(df, features, target='VIX', lookback=10, scale_method='MinMax'):
    df_clean = df.dropna(subset=features+[target]).copy()
    df_clean['regime'] = df_clean.index.to_series().apply(label_market_regime)
    df_clean['regime_type'] = df_clean['regime'].apply(categorize_regime_type)
    
    if len(df_clean) <= lookback:
        return None, None, None, None, None
    
    scaler = StandardScaler() if scale_method=='Standard' else MinMaxScaler()
    scaled = scaler.fit_transform(df_clean[features + [target]])
    
    X, y, idx, regimes = [], [], [], []
    for i in range(lookback, len(scaled)):
        X.append(scaled[i-lookback:i, :-1])
        y.append(scaled[i, -1])
        idx.append(df_clean.index[i])
        regimes.append(df_clean['regime_type'].iloc[i])
    
    return np.array(X), np.array(y), idx, scaler, regimes

# ------------------------------------------------
# 11. Statistical Baselines: Enhanced ARIMA & GARCH
# ------------------------------------------------
def train_arima_baseline(y_series, max_p=3, max_d=2, max_q=3):
    """Enhanced ARIMA with automatic order selection"""
    best_aic = np.inf
    best_model = None
    best_order = None
    
    for p in range(max_p + 1):
        for d in range(max_d + 1):
            for q in range(max_q + 1):
                try:
                    model = ARIMA(y_series, order=(p,d,q))
                    fitted_model = model.fit()
                    if fitted_model.aic < best_aic:
                        best_aic = fitted_model.aic
                        best_model = fitted_model
                        best_order = (p,d,q)
                except:
                    continue
    
    return best_model if best_model else ARIMA(y_series, order=(1,0,1)).fit()

def train_garch_baseline(y_series, max_p=2, max_q=2):
    """Enhanced GARCH with automatic order selection"""
    best_aic = np.inf
    best_model = None
    
    for p in range(1, max_p + 1):
        for q in range(1, max_q + 1):
            try:
                model = arch_model(y_series, vol='Garch', p=p, q=q, dist='normal')
                fitted_model = model.fit(disp='off')
                if fitted_model.aic < best_aic:
                    best_aic = fitted_model.aic
                    best_model = fitted_model
            except:
                continue
    
    return best_model if best_model else arch_model(y_series, vol='Garch', p=1, q=1).fit(disp='off')

# ------------------------------------------------
# 12. Enhanced Model Architecture
# ------------------------------------------------
def squash(vectors, axis=-1):
    s2n = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)
    scale = s2n / (1 + s2n) / tf.sqrt(s2n + K.epsilon())
    return scale * vectors

def build_capsule_model(input_shape, num_capsule=10, dim_capsule=16):
    inputs = Input(shape=input_shape)
    x = Conv1D(128,3,activation='relu',padding='same')(inputs)
    x = MaxPooling1D(2)(x)
    x = Conv1D(256,3,activation='relu',padding='same')(x)
    x = MaxPooling1D(2)(x)
    x = Flatten()(x)
    x = Reshape((-1,dim_capsule))(x)
    x = Lambda(squash)(x)
    caps = [Lambda(squash)(TimeDistributed(Dense(dim_capsule))(x)) for _ in range(num_capsule)]
    net = concatenate(caps,axis=-1)
    net = Flatten()(net)
    net = Dense(64,activation='relu')(net)
    net = Dense(32,activation='relu')(net)
    out = Dense(1)(net)
    model = Model(inputs, out)
    model.compile(optimizer=Adam(), loss='mse')
    return model

def build_model(model_type, input_shape, layers=None, lr=1e-3, dropout_rate=0.2):
    if model_type=='CapsNet':
        return build_capsule_model(input_shape)
    elif model_type.startswith('Lightweight_'):
        return create_lightweight_model(model_type, input_shape)
    
    model = Sequential()
    
    if layers is None:
        configs = {
            'ANN': [128,64,32],
            'RNN': [128,64], 'LSTM': [128,64], 'GRU': [128,64],
            'CNN': [128,64], 'CNN_LSTM': [64,128,64]
        }
        layers = configs.get(model_type, [64,32])
    
    if model_type=='ANN':
        model.add(Input(shape=input_shape))
        model.add(Flatten())
        for units in layers:
            model.add(Dense(units,activation='relu'))
            model.add(Dropout(dropout_rate))
        model.add(Dense(1))
        
    elif model_type in ['RNN','LSTM','GRU']:
        LayerClass = {'RNN': SimpleRNN,'LSTM':LSTM,'GRU':GRU}[model_type]
        model.add(LayerClass(layers[0],return_sequences=True,input_shape=input_shape,dropout=dropout_rate))
        if len(layers) > 1:
            model.add(LayerClass(layers[1],dropout=dropout_rate))
        else:
            model.add(LayerClass(64,dropout=dropout_rate))
        model.add(Dense(1))
        
    elif model_type=='CNN':
        model.add(Conv1D(layers[0],3,activation='relu',input_shape=input_shape))
        model.add(MaxPooling1D(2))
        model.add(Conv1D(layers[1] if len(layers)>1 else 64,3,activation='relu'))
        model.add(MaxPooling1D(2))
        model.add(Flatten())
        model.add(Dropout(dropout_rate))
        model.add(Dense(1))
        
    elif model_type=='CNN_LSTM':
        model.add(Conv1D(layers[0] if len(layers)>0 else 64,3,activation='relu',input_shape=input_shape))
        model.add(MaxPooling1D(2))
        model.add(LSTM(layers[1] if len(layers)>1 else 128,return_sequences=True,dropout=dropout_rate))
        model.add(LSTM(layers[2] if len(layers)>2 else 64,dropout=dropout_rate))
        model.add(Dense(1))
    
    optimizer = Adam(learning_rate=lr)
    model.compile(optimizer=optimizer, loss='mse')
    return model

# ------------------------------------------------
# 13. Enhanced Metrics & Statistical Tests
# ------------------------------------------------
def calculate_metrics(y_true,y_pred):
    return {
        'mse': mean_squared_error(y_true,y_pred),
        'rmse': np.sqrt(mean_squared_error(y_true,y_pred)),
        'mae': mean_absolute_error(y_true,y_pred),
        'r2': r2_score(y_true,y_pred),
        'explained_variance': explained_variance_score(y_true,y_pred),
        'mape': np.mean(np.abs((y_true-y_pred)/(y_true+1e-8)))*100
    }

def diebold_mariano_test(y_true,y_pred1,y_pred2,crit='MSE'):
    e1,e2=y_true-y_pred1,y_true-y_pred2
    d=(e1**2)-(e2**2)
    DM=d.mean()/np.sqrt(d.var(ddof=1)/len(d))
    p=2*(1-0.5*(1+tf.math.erf(abs(DM)/tf.sqrt(2.0))))
    return DM,p

def compare_models_dm(pred_df, group_by_cols):
    """Enhanced DM test with regime-specific analysis"""
    dm_results = []
    
    for group_vals, group_data in pred_df.groupby(group_by_cols):
        models = group_data['model'].unique()
        
        for i, model1 in enumerate(models):
            for model2 in models[i+1:]:
                data1 = group_data[group_data['model'] == model1]
                data2 = group_data[group_data['model'] == model2]
                
                common_idx = set(data1['time_index']).intersection(set(data2['time_index']))
                if len(common_idx) < 10:  # Minimum sample size
                    continue
                    
                data1_aligned = data1[data1['time_index'].isin(common_idx)].sort_values('time_index')
                data2_aligned = data2[data2['time_index'].isin(common_idx)].sort_values('time_index')
                
                if len(data1_aligned) != len(data2_aligned):
                    continue
                
                try:
                    dm_stat, p_val = diebold_mariano_test(
                        data1_aligned['y_true'].values,
                        data1_aligned['y_pred'].values,
                        data2_aligned['y_pred'].values
                    )
                    
                    result_dict = dict(zip(group_by_cols, group_vals if isinstance(group_vals, tuple) else [group_vals]))
                    result_dict.update({
                        'model1': model1,
                        'model2': model2,
                        'dm_stat': float(dm_stat),
                        'p_value': float(p_val),
                        'n_obs': len(data1_aligned),
                        'significant': float(p_val) < 0.05
                    })
                    dm_results.append(result_dict)
                except Exception as e:
                    print(f"Error in DM test for {model1} vs {model2}: {e}")
                    continue
    
    return pd.DataFrame(dm_results)

# ------------------------------------------------
# 14. Enhanced Training & Evaluation
# ------------------------------------------------
def grid_search_model(X_train,y_train,X_val,y_val,model_type):
    """Enhanced grid search with multiple metrics"""
    best_cfg,best_score=None,np.inf
    search_results = []
    
    if model_type not in param_grid:
        return None, np.inf
    
    for cfg in ParameterGrid(param_grid[model_type]):
        try:
            if model_type == 'ANN':
                m = build_model(model_type, X_train.shape[1:], 
                              layers=cfg['layers'], lr=cfg['learning_rate'],
                              dropout_rate=cfg.get('dropout_rate', 0.2))
            elif model_type in ['LSTM', 'GRU']:
                m = build_model(model_type, X_train.shape[1:], 
                              layers=cfg['units'], lr=cfg['learning_rate'],
                              dropout_rate=cfg.get('dropout_rate', 0.2))
            else:
                m = build_model(model_type, X_train.shape[1:], lr=cfg['learning_rate'])
            
            hist = m.fit(X_train, y_train, epochs=30, batch_size=cfg.get('batch_size', 64),
                        validation_data=(X_val, y_val), verbose=0,
                        callbacks=[EarlyStopping(patience=5, restore_best_weights=True)])
            
            val_loss = min(hist.history['val_loss'])
            
            search_results.append({
                'config': cfg,
                'val_loss': val_loss,
                'train_loss': hist.history['loss'][-1]
            })
            
            if val_loss < best_score:
                best_score = val_loss
                best_cfg = cfg
                
        except Exception as e:
            print(f"Error in grid search for config {cfg}: {e}")
            continue
    
    return best_cfg, best_score

def train_and_evaluate_with_preds(idx,X,y,model_type,regimes=None,epochs=50,batch_size=64):
    """Enhanced training with regime analysis and interpretability"""
    split=int(len(X)*0.8)
    X_tr,y_tr,X_te,y_te = X[:split],y[:split],X[split:],y[split:]
    idx_te = idx[split:]
    regimes_te = regimes[split:] if regimes else None
    
    # Initialize profiler
    profiler = ModelLatencyProfiler()
    
    # Grid search for selected models
    if model_type in param_grid:
        cfg, _ = grid_search_model(X_tr,y_tr,X_te,y_te,model_type)
        if cfg:
            if model_type == 'ANN':
                model = build_model(model_type, X_tr.shape[1:], 
                                  layers=cfg['layers'], lr=cfg['learning_rate'],
                                  dropout_rate=cfg.get('dropout_rate', 0.2))
                batch_size = cfg['batch_size']
            elif model_type in ['LSTM', 'GRU']:
                model = build_model(model_type, X_tr.shape[1:], 
                                  layers=cfg['units'], lr=cfg['learning_rate'],
                                  dropout_rate=cfg.get('dropout_rate', 0.2))
                batch_size = cfg.get('batch_size', 64)
            else:
                model = build_model(model_type, X_tr.shape[1:], lr=cfg['learning_rate'])
                batch_size = cfg.get('batch_size', 64)
        else:
            model = build_model(model_type, X_tr.shape[1:])
    else:
        model = build_model(model_type, X_tr.shape[1:])
    
    # Training with callbacks
    callbacks = [
        EarlyStopping(patience=15, restore_best_weights=True),
        ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6)
    ]
    
    history = model.fit(X_tr, y_tr, epochs=epochs, batch_size=batch_size,
                       verbose=0, validation_split=0.2, callbacks=callbacks)
    
    # Predictions and metrics
    y_pred = model.predict(X_te).flatten()
    mets = calculate_metrics(y_te, y_pred)
    
    # Measure inference latency
    latency_stats = profiler.measure_inference_time(model, X_te, model_type)
    mets.update({f'latency_{k}': v for k, v in latency_stats.items() if k != 'model'})
    
    # Training metrics
    mets['train_loss'] = history.history['loss'][-1]
    mets['val_loss'] = history.history['val_loss'][-1] if 'val_loss' in history.history else np.nan
    mets['epochs_trained'] = len(history.history['loss'])
    
    # Feature importance analysis
    try:
        feature_names = [f'feature_{i}' for i in range(X_tr.shape[-1])]
        importance_scores, shap_values = calculate_feature_importance_shap(model, X_te, feature_names)
        if importance_scores is not None:
            visualize_feature_importance(importance_scores, feature_names, f"{model_type}_Feature_Importance")
    except Exception as e:
        print(f"Feature importance analysis failed for {model_type}: {e}")
    
    # Visualization
    plt.figure(figsize=(15, 10))
    
    # Forecast plot
    plt.subplot(2, 2, 1)
    plt.plot(idx_te, y_te, label='True', alpha=0.7)
    plt.plot(idx_te, y_pred, label='Pred', alpha=0.7)
    plt.legend()
    plt.title(f'{model_type} Forecast vs True')
    plt.xticks(rotation=45)
    
    # Error distribution
    plt.subplot(2, 2, 2)
    errs = y_te - y_pred
    sns.histplot(errs, kde=True)
    plt.title(f'{model_type} Error Distribution')
    plt.xlabel('Prediction Error')
    
    # Loss curves
    plt.subplot(2, 2, 3)
    plt.plot(history.history['loss'], label='Train Loss')
    if 'val_loss' in history.history:
        plt.plot(history.history['val_loss'], label='Val Loss')
    plt.legend()
    plt.title(f'{model_type} Loss Curves')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    
    # Regime-specific performance if available
    plt.subplot(2, 2, 4)
    if regimes_te:
        regime_performance = {}
        unique_regimes = list(set(regimes_te))
        for regime in unique_regimes:
            regime_mask = [r == regime for r in regimes_te]
            if sum(regime_mask) > 0:
                regime_mse = mean_squared_error(
                    np.array(y_te)[regime_mask], 
                    np.array(y_pred)[regime_mask]
                )
                regime_performance[regime] = regime_mse
        
        plt.bar(regime_performance.keys(), regime_performance.values())
        plt.title(f'{model_type} MSE by Market Regime')
        plt.xticks(rotation=45)
        plt.ylabel('MSE')
    else:
        plt.text(0.5, 0.5, 'No regime data available', ha='center', va='center', transform=plt.gca().transAxes)
    
    plt.tight_layout()
    plt.savefig(f'plots/{model_type}_comprehensive_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Create prediction dataframe with regime info
    pred_df = pd.DataFrame({
        'time_index': idx_te,
        'y_true': y_te,
        'y_pred': y_pred
    })
    
    if regimes_te:
        pred_df['regime'] = regimes_te
    
    return mets, pred_df

# ------------------------------------------------
# 15. Enhanced Benchmark Function
# ------------------------------------------------
def benchmark_all_combinations():
    """Enhanced benchmarking with comprehensive analysis"""
    base_cols = ['DIX','GEX','SKEW','PUTCALLRATIO']
    models = ['ARIMA','GARCH','ANN','RNN','LSTM','GRU','CNN','CNN_LSTM','CapsNet',
              'Lightweight_ANN', 'Lightweight_LSTM', 'Lightweight_CNN_LSTM']
    
    all_preds = []
    results = []
    shock_analyses = []
    profiler = ModelLatencyProfiler()
    
    df0 = load_data('merged_market_data_vix.csv')
    
    # Feature selection analysis
    print("Performing feature selection analysis...")
    feature_selection_results = []
    
    for r in range(1, len(base_cols)+1):
        for combo in itertools.combinations(base_cols, r):
            print(f"Processing feature combination: {combo}")
            
            for fractal in ['none','hurst','wavelet']:
                df = df0.copy()
                
                # Apply fractal/wavelet features
                if fractal == 'hurst':
                    df = apply_hurst(df)
                if fractal == 'wavelet':
                    df, _ = apply_wavelets(df)
                
                # Prepare feature list
                feats = list(combo)
                if fractal == 'hurst':
                    feats += ['HURST_PRICE']
                if fractal == 'wavelet':
                    feats += [c for c in df.columns if c.startswith('WAVELET_')]
                
                # Clean data
                df.dropna(subset=feats+['VIX'], inplace=True)
                df.reset_index(inplace=True)
                
                # Prepare features with regime information
                X, y, idx, scaler, regimes = prepare_features(df, feats, 'VIX')
                if X is None:
                    continue
                
                # Feature selection using RFE
                try:
                    selected_features, rankings, cv_scores = recursive_feature_elimination(X, y, feats)
                    feature_selection_results.append({
                        'feature_combination': '+'.join(combo),
                        'fractal_type': fractal,
                        'selected_features': selected_features,
                        'n_selected': len(selected_features),
                        'cv_score': np.mean(cv_scores)
                    })
                except Exception as e:
                    print(f"RFE failed for {combo}, {fractal}: {e}")
                
                # Train and evaluate models
                for model_name in models:
                    print(f"  Training {model_name}...")
                    
                    try:
                        if model_name == 'ARIMA':
                            series = df['VIX']
                            split = int(len(series) * 0.8)
                            m_ar = train_arima_baseline(series[:split])
                            
                            # Forecast
                            forecast_steps = len(series) - split
                            pred = m_ar.forecast(steps=forecast_steps)
                            
                            mets = calculate_metrics(series.iloc[split:], pred.values)
                            pred_df = pd.DataFrame({
                                'time_index': series.index[split:],
                                'y_true': series.iloc[split:],
                                'y_pred': pred.values
                            })
                            
                        elif model_name == 'GARCH':
                            series = df['VIX']
                            split = int(len(series) * 0.8)
                            m_g = train_garch_baseline(series[:split])
                            
                            # Forecast volatility
                            forecast_steps = len(series) - split
                            fore = m_g.forecast(horizon=forecast_steps, reindex=False)
                            vol_pred = np.sqrt(fore.variance.values[-1])
                            
                            # Repeat prediction for all forecast steps
                            vol_pred_array = np.full(forecast_steps, vol_pred)
                            
                            mets = calculate_metrics(series.iloc[split:], vol_pred_array)
                            pred_df = pd.DataFrame({
                                'time_index': series.index[split:],
                                'y_true': series.iloc[split:],
                                'y_pred': vol_pred_array
                            })
                            
                        else:
                            # Neural network models
                            mets, pred_df = train_and_evaluate_with_preds(
                                idx, X, y, model_name, regimes
                            )
                        
                        # Add regime information to predictions
                        if 'regime' not in pred_df.columns and regimes:
                            split = int(len(regimes) * 0.8)
                            pred_df['regime'] = regimes[split:]
                        
                        # Store results
                        result_entry = {
                            'features': '+'.join(combo),
                            'model': model_name,
                            'fractal': fractal,
                            **mets
                        }
                        results.append(result_entry)
                        
                        # Add metadata to predictions
                        pred_df = pred_df.assign(
                            features='+'.join(combo),
                            model=model_name,
                            fractal=fractal
                        )
                        all_preds.extend(pred_df.to_dict('records'))
                        
                    except Exception as e:
                        print(f"Error training {model_name}: {e}")
                        continue
    
    # Save results
    results_df = pd.DataFrame(results)
    results_df.to_csv('results/combo_results_enhanced.csv', index=False)
    
    all_preds_df = pd.DataFrame(all_preds)
    
    # Enhanced DM tests
    print("Performing Diebold-Mariano tests...")
    dm_df = compare_models_dm(all_preds_df, ['features','fractal'])
    dm_df.to_csv('results/dm_results_enhanced.csv', index=False)
    
    # Market shock analysis
    print("Analyzing market shock scenarios...")
    shock_analysis_df = analyze_market_shock_scenarios(all_preds_df)
    shock_analysis_df.to_csv('results/shock_analysis.csv', index=False)
    
    # Save feature selection results
    feature_selection_df = pd.DataFrame(feature_selection_results)
    feature_selection_df.to_csv('results/feature_selection_analysis.csv', index=False)
    
    # Model efficiency comparison
    print("Creating efficiency comparison...")
    profiler.compare_model_efficiency()
    
    # Generate comprehensive interpretability report for best models
    print("Generating interpretability analysis...")
    try:
        # Find best performing models
        best_models = results_df.nlargest(3, 'r2')
        
        for _, best_model in best_models.iterrows():
            # Get corresponding predictions
            model_preds = all_preds_df[
                (all_preds_df['model'] == best_model['model']) &
                (all_preds_df['features'] == best_model['features']) &
                (all_preds_df['fractal'] == best_model['fractal'])
            ]
            
            if len(model_preds) > 0:
                print(f"Generating interpretability for {best_model['model']}")
                # Note: Full interpretability analysis would require the trained model
                # This is a placeholder for the structure
                
    except Exception as e:
        print(f"Interpretability analysis failed: {e}")
    
    print("Enhanced benchmark analysis complete!")
    return results_df, all_preds_df, dm_df, shock_analysis_df

# ------------------------------------------------
# 16. Main execution
# ------------------------------------------------
if __name__ == '__main__':
    print("Starting enhanced benchmark analysis...")
    results_df, preds_df, dm_df, shock_df = benchmark_all_combinations()
    gc.collect()
    print("Analysis complete. Results saved to 'results/' directory.")
